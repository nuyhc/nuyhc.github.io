---
title: "멋쟁이 사자처럼 AI School 12주차"
categories: [Retrospective, AI School]
tags:
    - likelion
---

## 7월 18일
- 자연어 처리 (NLP)
- (주로) 텍스트를 벡터화하는 방법
  - 머신러닝: 단어의 빈도수
  - 딥러닝: 시퀀스 방식의 인코딩
- 정규 표현식
- 토큰화(Tokenization):
  - 텍스트 조각을 토큰이라 함
  - 패턴을 찾는데 유용
- 정제(Cleaning): 노이즈 제거
- 정규화(Normalization): 표협 방법이 다른 단어들을 같은 단어로 만듦
- 어간 추출(Stemming): 단어 형식을 의미가 있거나 무의미할 수 있는 줄기로 축소 (원형을 유지하지 않음)
- 표제어 표기법(Lemmatization): 언어학적으로 유효한 의미로 축소 (원형을 유지함)
- `n-gram`은 CNN에서 주변 정보를 이용하는것과 유사함
- `BOW(Back Of Word)`에서 순서를 고려하지 않는 단점을 보완 -> `N-Gram`
- 단어 빈도만 고려했을 경우, 불용어의 가능성이 높아짐

## 7월 19일
- 텍스트 데이터 EDA 및 전처리
- `CountVectorizer`에서 불용어를 제거하는 효과 -> `max_df`
- 오타나 희귀 단어를 제거화는 효과 -> `min_df`
- 정규 표현식
- `wordcloud`
- `Stemming(어간추출)`은 원형을 잃어버릴 수 있음
- `Lemmatization(표제어 표기법)`은 원형을 보전

## 7월 20일
- NLP
- `strmming`의 경우 활용형들을 원형 하나로 변환해 용량이 줄어듬
- RNN (순환 신경망)
  - one to one
  - one to many
  - many to one
  - many to many
- 시퀀스 데이터 (sequence data)
- 셀(cell): RNN의 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드 (RNN의 반복 단위, 개별)
- 메모리셀(memory cell): 이전의 값을 기억하는 셀 (RNNcell, 전체)
- 은닉상태(hidden state): 은닉층의 메모리 셀에서 나온 값이 출력층 방향 또는 다음 시점의 자신에게 보내는 상태
- BPTT(Back-Propagation Through Time)
  - 타임 스텝별로 네트워크를 펼친 후 사용
  - 현재 시간의 오차를 과거 시간의 상태까지 역전파
- 워드 임베딩(Word Embeding): 단어를 특정 차원의 벡터로 바꾸어 주는 것 -> 벡터화
- 시퀀스 방식의 인코딩 -> 패딩
- NLP의 첫 층은 임베딩(Embedding) 층

## 7월 21일
## 7월 22일
- Deep Learning Project

## 요약
- NLP는 정말 못하겠음.. 이해도 잘 안되고 흥미도 별로 없고..
- 프로젝트하면서 디텍션 API를 사용해봤는데 흥미가 생김