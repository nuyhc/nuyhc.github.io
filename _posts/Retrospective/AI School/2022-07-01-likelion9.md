---
title: "멋쟁이 사자처럼 AI Shcool 9주차"

categories: [Retrospective, AI School]
tags:
    - likelion
---

## 6월 27일
- 범위 변환(Scaling)
  - `fit`은 `train`에만 진행하고, `test`에는 진행하지 않음
  - `transform`은 양쪽 모두에 적용
  - `fit_transform`을하고 `train`에만 `transform`을하는 방식
  - 2개의 데이터가 합쳐져있는 경우에는 함께 진행
  - 각각의 평균과 표준편차가 다르기 때문에, 0과 1로 변환된 평균과 표준편차 값이 서로 다른 값을 의미하게 됨
- 범주화(Binning): 연속형 변수 -> 범주형 변수  
  - Equal width binning (`_cut`)  
     - 가능한 값의 범위를 동일한 너비의 N개의 빈으로 나눔, 편향된 분포에 민감  
  - Equal frequency binning (`_qcut`)  
     - 변수의 가능한 값 범위를 N개의 빈으로 나눔 (각 빈은 동일한 양의 관측값), 알고리즘의 성능을 올리지만 관계를 방해할 가능성이 있음
- 숫자화(Dummy): 범주형 변수 -> 연속형 변수
   - 인코딩(Encoding) -> Ordianl-Encoding, One-hot Encoding
     - Categorical Feature의 고유값들을 임의의 숫자로 바꿈 (Ordinal-Encoding)
- 정규분포 형태가 모델 학습에 도움이 됨
- 파생변수 생성 (Feature Generation)
   - 이미 존재하는 변수로부터 여러가지 방법을 이용해 새로운 변수를 만들어낼 수 있음
   - 적합한 파생변수는 모델 성능 향상에 도움이 됨

## 6월 28일
- `None`으로 결측치를 대체하는 경우, 인코딩시 **없다**라는 정보도 정보로 취급 가능
- 연속된 수치 데이터도 인코딩이 가능함 -> 수치 데이터 개수만큼 컬럼이 생김, **순서성(원의미)**을 잃어버림
- 수치형 데이터여도 특정부분에 몰려있을 경우, 변환이 의미가 없을 수 있음
- `from sklearn.model_selection import KFold`로 `cross_val_predict`의 `cv = KFold`를 사용하면,
  - shuffle 여부 선택 가능
  - 샘플링 시드 부여 가능
- `Gradiant Boosting`: 앙상블의 부스팅 계열 알고리즘
  - GBM(Gradient Boosting Machine)
  - XGBoost
  - LightGBM
  - catboost
- 전체에 대해 구하는 경우 -> err
- 표본(sample)을 통해 구하는 경우 -> residual(잔차)
- `MSE`는 이상치에 민감함 -> 보완 -> `Huber loss`, `Quantile loss`
- `MAE`를 사용하는 경우, 방향은 다르나 기울기가 같은 값만 나와 잘 사용하지 않음 (부호가 바뀌기전까지 기울기가 유지)
- 제조업에서 머신러닝의 필요성
  - 품질 관리
  - 예방 정비
  - 수요 예측
  - 프로세싱 조건
  - 연구 개발
  - 스마트 제품
- 선형회귀 모델
  - 간단한 작동 원리, 빠른 학습 속도, 조절할 파라미터 양이 적음, 이상치 영향이 큼
  - 수치형 변수만 있거나 경향성이 뚜렷한 경우 좋음
  - 보완한 모델
       - Ridge
       - Lasso
       - ElasticNet  
- 배깅은 비상관화 시켜주는 과정  
- 오버피팅이 있을때, 배깅이 더 좋은 성능을 보이는 경우가 있음  
  - 트리들이 비상관화 되어 있어서  
- 히스토그램 기반 알고리즘을 사용하는 이유  
  - 데이터 추출시 고른 데이터 샘플을 얻을 수 있음  
  - 결측치 처리가 필요 없어짐  
- 범주형 데이터가 많은 경우 `catboost`가 좋은 성능을 보임  

## 6월 29일
- 랜덤포레스트 (배깅) -> 무작위성
- 그래디언트 부스팅 트리 -> 무작위성 없이, 이전 트리를 개선하는 방식
- GOSS(Gradient bassed One Side Sampling) -> 행을 어떻게 줄일 것인지
- EFB(Exclusive Feature Bundling) -> 열을 어떻게 줄일 것인지
- CatBoost: 대칭트리, 예측 시간 감소
- XGBoost: BFS처럼 넓게 형성
- LightGBM: DFS처럼 깊게 형성
- Confusion Matrix (혼돈 행렬)
  - precision(정밀도): 1로 예측한것중 진짜 1인것
  - recall(재현율): 실제값이 1인것 중에 맞게 추론한것
  - specificity(특이도)
- Resampling
  - Oversampling (`imbalanced-learn`)
       - SMOTE(Synthetic Minority Over-sampling Technique)
       - 합성 소수자 오버 샘플링 기법
       - K-근접 이웃 이용
  - Undersampling
       - 구현은 쉽지만, 성능 저하 가능성이 있음
       - 부트스트래핑  
- `ravel` / `flatten`  

## 6월 30일
- 추천 이력을 저장하고 이용하기
- 파일 시스템 : 정보를 디스크에 저장하는 단위


```python
# 파일 생성
# a: 파일이 존재하면 뒤에 이어 붙이고, 없으면 생성한 뒤 write
with open("test.txt", "w") as f:
  f.write("~~~~")

# 배열처럼 인식해, 한줄씩 읽어옴
with open("test.txt", "r") as f:
  for line in f:
    print(line)
```


- 파이프
- 입출력전환
- 가치가 만들어지는 순사로 작업하기
  - 빨리 개발하기 X
  - 변환에 잘 대응하기 O

## 7월 1일
- Tableau 특강

## 요약
- 기본적인 ML 개념들은 다 배운거 같음
- 소음과 잡음을 구별하는건 여전히 어려운거 같음
- Confusion Matrix는 볼 때마다 어려운데 익숙해지는거 같기는 함..